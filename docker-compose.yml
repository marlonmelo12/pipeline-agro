# AGRO_PIPELINE/docker-compose.yml (VERSÃO FINAL CORRIGIDA)

services:
  # --- 1. INFRAESTRUTURA CENTRAL ---
  kafka:
    image: bitnami/kafka:latest
    container_name: kafka
    ports:
      - "9092:9092"
    networks: [pipeline_network]
    environment:
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9093
      - KAFKA_CFG_LISTENERS=INTERNAL://:29092,EXTERNAL://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka:29092,EXTERNAL://localhost:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL
      - ALLOW_PLAINTEXT_LISTENER=yes
    healthcheck:
      test: ["CMD-SHELL", "/opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s

  minio:
    image: minio/minio:latest
    container_name: minio
    ports: ["9000:9000", "9001:9001"]
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
    command: server /data --console-address ":9001"
    volumes: [minio_data:/data]
    networks: [pipeline_network]
  
  postgres-db:
    image: postgres:14-alpine
    container_name: postgres-db
    ports: ["5432:5432"]
    environment:
      - POSTGRES_USER=agro_user
      - POSTGRES_PASSWORD=agro_password
      - POSTGRES_DB=agro_db
    volumes: [postgres_agro_data:/var/lib/postgresql/data]
    networks: [pipeline_network]

  # Adicione o novo Data Warehouse PostgreSQL aqui, em sua própria seção
  postgres-dw:
    image: postgres:14-alpine
    container_name: postgres_dw_service
    ports:
      - "5434:5432"
    environment:
      - POSTGRES_USER=dw_user
      - POSTGRES_PASSWORD=dw_password
      - POSTGRES_DB=dw_agro
    volumes:
      - postgres_dw_data:/var/lib/postgresql/data
    networks: [pipeline_network]

  # --- 2. FONTES E PRODUTORES DE STREAMING ---
  api_mock:
    build:
      context: ./api_mock
    container_name: api_mock_service
    ports: ["5001:5001"]
    volumes: ["./api_mock/api_data:/app/api_data"]
    networks: [pipeline_network]

  api_stream_producer:
    build:
      context: ./producers/api_stream_producer
    container_name: api_stream_producer
    depends_on:
      kafka:
        condition: service_healthy
      api_mock:
        condition: service_started
    networks: [pipeline_network]
  
  commodities_stream_producer:
    build:
      context: ./producers/commodities_stream_producer
    container_name: commodities_stream_producer
    depends_on:
      kafka:
        condition: service_healthy
    volumes: ["./producers/commodities_stream_producer/data:/app/data"]
    networks: [pipeline_network]
  
  sql_stream_producer:
    build:
      context: ./producers/sql_stream_producer
    container_name: sql_stream_producer
    depends_on:
      kafka:
        condition: service_healthy
      postgres-db:
        condition: service_started
    networks: [pipeline_network]

  # --- 3. CONSUMIDORES (SALVAM NO MINIO) ---
  climate_data_saver:
    build:
      context: ./consumers/climate_saver
    container_name: climate_data_saver
    depends_on:
      kafka:
        condition: service_healthy
      minio:
        condition: service_started
    networks: [pipeline_network]
  
  commodities_data_saver:
    build:
      context: ./consumers/commodities_saver
    container_name: commodities_data_saver
    depends_on:
      kafka:
        condition: service_healthy
      minio:
        condition: service_started
    networks: [pipeline_network]

  production_data_saver:
    build:
      context: ./consumers/production_saver
    container_name: production_data_saver
    depends_on:
      kafka:
        condition: service_healthy
      minio:
        condition: service_started
    networks: [pipeline_network]
  
  # --- 4. CAMADA DE TRANSFORMAÇÃO SPARK ---
  spark-job-commodities:
    build:
      context: ./spark
    container_name: spark_job_commodities
    depends_on:
      - minio
    networks: [pipeline_network]
    command: >
      /opt/bitnami/spark/bin/spark-submit
      --jars /opt/bitnami/spark/jars/delta-spark_2.12-3.1.0.jar
      jobs/transform_commodities.py

  spark-job-climate:
    build:
      context: ./spark
    container_name: spark_job_climate
    depends_on:
      - minio
    networks: [pipeline_network]
    command: >
      /opt/bitnami/spark/bin/spark-submit
      --jars /opt/bitnami/spark/jars/delta-spark_2.12-3.1.0.jar
      jobs/transform_climate.py

  spark-job-production:
    build:
      context: ./spark
    container_name: spark_job_production
    depends_on:
      - minio
    networks: [pipeline_network]
    command: >
      /opt/bitnami/spark/bin/spark-submit
      --jars /opt/bitnami/spark/jars/delta-spark_2.12-3.1.0.jar
      jobs/transform_production.py
      
  spark-gold-job:
    build:
      context: ./spark
    container_name: spark_gold_job
    depends_on:
      - minio
      - postgres-dw
    networks: [pipeline_network]
    command: >
      /opt/bitnami/spark/bin/spark-submit
      --jars /opt/bitnami/spark/jars/delta-spark_2.12-3.1.0.jar,/opt/bitnami/spark/jars/delta-storage-3.1.0.jar,/opt/bitnami/spark/jars/postgresql-42.7.7.jar
      --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension"
      --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"
      jobs/create_gold_tables.py
  
  dashboard:
    build:
      context: ./dashboard
    container_name: dashboard
    ports:
      - "8501:8501"
    depends_on:
      - postgres-dw
    networks: [pipeline_network]

# --- 5. DECLARAÇÃO DA REDE E DOS VOLUMES ---
networks:
  pipeline_network:
    driver: bridge

volumes:
  minio_data:
  postgres_agro_data:
  postgres_dw_data: