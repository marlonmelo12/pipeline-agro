# AGRO_PIPELINE/docker-compose.yml (VERSÃO FINAL COMPLETA E ESTÁVEL)

services:
  # --- 1. INFRAESTRUTURA CENTRAL ---
  kafka:
    image: bitnami/kafka:latest
    container_name: kafka
    ports:
      - "9092:9092"
    networks: [pipeline_network]
    environment:
      # Configuração do Modo KRaft (sem Zookeeper)
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9093
      # Configuração dos Listeners para comunicação interna e externa
      - KAFKA_CFG_LISTENERS=INTERNAL://:29092,EXTERNAL://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka:29092,EXTERNAL://localhost:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL
      - ALLOW_PLAINTEXT_LISTENER=yes
    healthcheck:
      test: ["CMD-SHELL", "/opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s

  minio:
    image: minio/minio:latest
    container_name: minio
    ports: ["9000:9000", "9001:9001"]
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
    command: server /data --console-address ":9001"
    volumes: [minio_data:/data]
    networks: [pipeline_network]
  
  postgres-db:
    image: postgres:14-alpine
    container_name: postgres-db
    ports: ["5432:5432"]
    environment:
      - POSTGRES_USER=agro_user
      - POSTGRES_PASSWORD=agro_password
      - POSTGRES_DB=agro_db
    volumes: [postgres_agro_data:/var/lib/postgresql/data]
    networks: [pipeline_network]
  
  # --- 2. FONTES E PRODUTORES DE STREAMING ---
  api_mock:
    build:
      context: ./api_mock
    container_name: api_mock_service
    ports: ["5001:5001"]
    volumes: ["./api_mock/api_data:/app/api_data"]
    networks: [pipeline_network]

  api_stream_producer:
    build:
      context: ./producers/api_stream_producer
    container_name: api_stream_producer
    depends_on:
      kafka:
        condition: service_healthy
      api_mock:
        condition: service_started
    networks: [pipeline_network]
  
  commodities_stream_producer:
    build:
      context: ./producers/commodities_stream_producer
    container_name: commodities_stream_producer
    depends_on:
      kafka:
        condition: service_healthy
    volumes: ["./producers/commodities_stream_producer/data:/app/data"]
    networks: [pipeline_network]
  
  sql_stream_producer:
    build:
      context: ./producers/sql_stream_producer
    container_name: sql_stream_producer
    depends_on:
      kafka:
        condition: service_healthy
      postgres-db:
        condition: service_started
    networks: [pipeline_network]

  # --- 3. CONSUMIDORES (SALVAM NO MINIO) ---
  climate_data_saver:
    build:
      context: ./consumers/climate_saver
    container_name: climate_data_saver
    depends_on:
      kafka:
        condition: service_healthy
      minio:
        condition: service_started
    networks: [pipeline_network]
  
  commodities_data_saver:
    build:
      context: ./consumers/commodities_saver
    container_name: commodities_data_saver
    depends_on:
      kafka:
        condition: service_healthy
      minio:
        condition: service_started
    networks: [pipeline_network]

  production_data_saver:
    build:
      context: ./consumers/production_saver
    container_name: production_data_saver
    depends_on:
      kafka:
        condition: service_healthy
      minio:
        condition: service_started
    networks: [pipeline_network]
  
  # ... (depois do seu último serviço "production_data_saver")

  # --- 4. CAMADA DE TRANSFORMAÇÃO ---
  spark-job-commodities:
    build:
      context: ./spark
    container_name: spark_job_commodities
    depends_on:
      - minio
    networks: [pipeline_network]
    # Este comando executa nosso script dentro do contêiner Spark
    command: spark-submit jobs/transform_commodities.py

  spark-job-climate:
    build:
      context: ./spark
    container_name: spark_job_climate
    depends_on:
      - minio
    networks: [pipeline_network]
    command: spark-submit jobs/transform_climate.py

  spark-job-production:
    build:
      context: ./spark
    container_name: spark_job_production
    depends_on:
      - minio
    networks: [pipeline_network]
    command: spark-submit jobs/transform_production.py


# --- 4. DECLARAÇÃO DA REDE E DOS VOLUMES ---
networks:
  pipeline_network:
    driver: bridge

volumes:
  minio_data:
  postgres_agro_data: